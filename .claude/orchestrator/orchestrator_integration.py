"""
Master Orchestrator Integration
Integrates all world-class components:
- Observability (tracing, metrics, dashboards)
- Self-healing (feedback loops, auto-retry)
- Memory (short/medium/long-term)
- Security (PII, code scanning)
- Evaluation (LLM-as-judge)
"""
from typing import Dict, Any, Optional, List
import logging
import uuid
from datetime import datetime
from pathlib import Path

# Import all components
from .observability.tracer import get_tracer
from .observability.metrics_collector import get_metrics_collector
from .observability.alerting import get_alerting_system, Alert, AlertSeverity
from .feedback.execution_tracker import get_execution_tracker
from .feedback.self_healer import get_self_healer
from .memory.memory_manager import get_memory_manager
from .security.security_manager import get_security_manager
from .evaluation.llm_judge import get_llm_judge

logger = logging.getLogger(__name__)


class WorldClassOrchestrator:
    """
    Integrated world-class orchestrator with all enterprise features
    - 100% Free
    - 100% Local
    - Production-ready
    """

    def __init__(self):
        # Initialize all components
        self.tracer = get_tracer()
        self.metrics = get_metrics_collector()
        self.alerting = get_alerting_system()
        self.tracker = get_execution_tracker()
        self.healer = get_self_healer()
        self.memory = get_memory_manager()
        self.security = get_security_manager()
        self.judge = get_llm_judge()

        logger.info("World-class orchestrator initialized")

    def execute_agent(
        self,
        agent_name: str,
        request: str,
        context: Dict[str, Any] = None,
        feature_name: str = None
    ) -> Dict[str, Any]:
        """
        Execute an agent with full observability and self-healing

        Args:
            agent_name: Name of the agent to execute
            request: The request/task for the agent
            context: Additional context
            feature_name: Feature being worked on

        Returns:
            Execution result with full telemetry
        """
        execution_id = str(uuid.uuid4())
        context = context or {}
        context['request_id'] = execution_id
        context['feature'] = feature_name

        # Step 1: Security validation
        logger.info(f"Validating input for {agent_name}...")
        validation = self.security.validate_input(request)
        if not validation['valid']:
            logger.error(f"Security validation failed: {validation['issues']}")
            return {
                "success": False,
                "error": "Security validation failed",
                "issues": validation['issues']
            }

        # Step 2: Check memory for context
        if feature_name:
            stored_context = self.memory.retrieve_context(feature_name)
            if stored_context:
                logger.info(f"Retrieved context for {feature_name}")
                context.update(stored_context)

        # Step 3: Start tracing
        span = self.tracer.start_agent_execution(agent_name, context)
        start_time = datetime.now()

        # Step 4: Execute with self-healing
        result = self._execute_with_healing(
            agent_name, request, context, execution_id
        )

        # Step 5: End tracing
        duration_ms = (datetime.now() - start_time).total_seconds() * 1000
        self.tracer.end_agent_execution(
            span,
            result['success'],
            result.get('tokens_used', 0),
            result.get('error')
        )

        # Step 6: Record metrics
        self.metrics.record_agent_execution(
            agent_name,
            duration_ms,
            result['success'],
            result.get('tokens_used', 0),
            result.get('model_name', 'local'),
            feature_name or 'unknown',
            result.get('error')
        )

        # Step 7: Track execution
        self.tracker.track_execution(
            execution_id,
            agent_name,
            result['success'],
            duration_ms,
            request,
            feature_name,
            result.get('tokens_used', 0),
            result.get('model_name', 'local'),
            result.get('error'),
            result.get('error_type'),
            context,
            result.get('files_modified', []),
            result.get('retry_count', 0)
        )

        # Step 8: Check for alerts
        if not result['success']:
            self.alerting.send_alert(Alert(
                severity=AlertSeverity.ERROR,
                title=f"Agent execution failed: {agent_name}",
                message=result.get('error', 'Unknown error'),
                context={"agent": agent_name, "execution_id": execution_id}
            ))

        # Step 9: Update memory
        if result['success'] and feature_name:
            self.memory.store_context(feature_name, {
                "last_execution": execution_id,
                "last_agent": agent_name,
                "timestamp": datetime.now().isoformat()
            })

        # Step 10: Sanitize output
        if 'output' in result:
            result['output'] = self.security.sanitize_output(result['output'])

        return result

    def _execute_with_healing(
        self,
        agent_name: str,
        request: str,
        context: Dict[str, Any],
        execution_id: str,
        retry_count: int = 0
    ) -> Dict[str, Any]:
        """Execute agent with self-healing retry logic"""

        # Simulate agent execution (replace with actual agent call)
        # In real implementation, this would call the actual agent
        try:
            # Placeholder for actual agent execution
            result = self._call_agent(agent_name, request, context)

            if result['success']:
                # Learn from success
                self._learn_from_success(agent_name, request, context)
                return result
            else:
                # Try to heal and retry
                return self._handle_failure(
                    agent_name, request, context, execution_id,
                    result, retry_count
                )

        except Exception as e:
            logger.error(f"Agent execution exception: {e}")
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "retry_count": retry_count
            }

    def _call_agent(self, agent_name: str, request: str,
                   context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Placeholder for actual agent call
        In production, this would integrate with your agent system
        """
        # This is where you'd actually call your agent
        # For now, return a mock success
        return {
            "success": True,
            "output": f"Agent {agent_name} processed: {request[:50]}...",
            "tokens_used": 100,
            "model_name": "local"
        }

    def _handle_failure(
        self,
        agent_name: str,
        request: str,
        context: Dict[str, Any],
        execution_id: str,
        result: Dict[str, Any],
        retry_count: int
    ) -> Dict[str, Any]:
        """Handle agent failure with self-healing"""

        error_message = result.get('error', 'Unknown error')
        error_type = result.get('error_type', 'Unknown')

        # Check if we can auto-heal
        if self.healer.can_auto_heal(error_message, error_type):
            logger.info(f"Attempting auto-heal for {error_type}")
            fix = self.healer.suggest_fix(error_message, error_type, context)

            if fix and fix.get('confidence', 0) > 0.8:
                logger.info(f"Auto-fix available: {fix['suggestion']}")
                # In real implementation, apply the fix
                # For now, just log it
                pass

        # Retry if not exceeded max retries
        if retry_count < self.healer.max_retries:
            logger.info(f"Retrying ({retry_count + 1}/{self.healer.max_retries})...")

            # Get retry strategy
            strategy = self.healer.suggest_retry_strategy(
                {"agent_name": agent_name, "error_type": error_type},
                retry_count
            )

            logger.info(f"Retry strategy: {strategy['description']}")

            # Retry with modified approach
            return self._execute_with_healing(
                agent_name, request, context, execution_id, retry_count + 1
            )
        else:
            logger.error(f"Max retries exceeded for {agent_name}")
            return result

    def _learn_from_success(self, agent_name: str, request: str,
                           context: Dict[str, Any]):
        """Learn from successful executions"""
        # Store successful pattern
        self.memory.store_knowledge(
            f"success_pattern_{agent_name}",
            {
                "agent": agent_name,
                "request_type": request[:100],
                "context_keys": list(context.keys()),
                "timestamp": datetime.now().isoformat()
            },
            category="patterns",
            confidence=0.8
        )

    def evaluate_code(self, code: str, language: str = "python") -> Dict[str, Any]:
        """Evaluate code quality using LLM-as-judge"""
        return self.judge.evaluate_code_quality(code, language)

    def get_dashboard_data(self, hours: int = 24) -> Dict[str, Any]:
        """Get data for dashboard generation"""
        metrics = self.metrics.get_summary(hours=hours)
        trends = self.metrics.get_performance_trends(days=7)
        stats = self.tracker.get_statistics(hours=hours)

        return {
            "metrics": metrics,
            "trends": trends,
            "execution_stats": stats,
            "recent_alerts": self.alerting.get_recent_alerts(hours=hours)
        }

    def generate_dashboard(self, hours: int = 24) -> str:
        """Generate HTML dashboard"""
        from .observability.dashboard_generator import generate_dashboard

        data = self.get_dashboard_data(hours)
        return generate_dashboard(
            data['metrics'],
            data['trends']
        )


# Singleton instance
_orchestrator: Optional[WorldClassOrchestrator] = None


def get_orchestrator() -> WorldClassOrchestrator:
    """Get singleton orchestrator instance"""
    global _orchestrator
    if _orchestrator is None:
        _orchestrator = WorldClassOrchestrator()
    return _orchestrator


# Convenience functions
def execute_agent(agent_name: str, request: str, **kwargs) -> Dict[str, Any]:
    """Convenience function to execute agent"""
    orchestrator = get_orchestrator()
    return orchestrator.execute_agent(agent_name, request, **kwargs)


def evaluate_code(code: str, language: str = "python") -> Dict[str, Any]:
    """Convenience function to evaluate code"""
    orchestrator = get_orchestrator()
    return orchestrator.evaluate_code(code, language)


def generate_dashboard(hours: int = 24) -> str:
    """Convenience function to generate dashboard"""
    orchestrator = get_orchestrator()
    return orchestrator.generate_dashboard(hours)


if __name__ == "__main__":
    # Test the integrated orchestrator
    import json

    print("\n" + "="*70)
    print("  TESTING WORLD-CLASS ORCHESTRATOR")
    print("="*70 + "\n")

    # Initialize
    orchestrator = get_orchestrator()
    print("âœ“ Orchestrator initialized\n")

    # Test 1: Execute agent
    print("Test 1: Execute agent...")
    result = orchestrator.execute_agent(
        "test-agent",
        "Test request for the agent",
        feature_name="test-feature"
    )
    print(f"Result: {json.dumps(result, indent=2)}\n")

    # Test 2: Evaluate code
    print("Test 2: Evaluate code...")
    test_code = """
def hello_world():
    print("Hello, World!")
    return True
"""
    evaluation = orchestrator.evaluate_code(test_code)
    print(f"Evaluation: {json.dumps(evaluation, indent=2)}\n")

    # Test 3: Generate dashboard
    print("Test 3: Generate dashboard...")
    dashboard_path = orchestrator.generate_dashboard()
    print(f"Dashboard generated: {dashboard_path}\n")

    # Test 4: Get statistics
    print("Test 4: Get statistics...")
    data = orchestrator.get_dashboard_data()
    print(f"Total executions: {data['metrics']['overall']['total_executions']}")
    print(f"Success rate: {data['metrics']['overall']['success_rate']:.1f}%\n")

    print("="*70)
    print("  ALL TESTS COMPLETE!")
    print("="*70)
